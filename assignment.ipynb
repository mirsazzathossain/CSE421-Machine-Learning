{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBahU3irSMb9"
      },
      "source": [
        "## **Classifying Wine Origins from Physicochemical Properties**\n",
        "\n",
        "### **Previous Resources**\n",
        "\n",
        "Link to the resources of lab class is given below:\n",
        "\n",
        "1. [Simple Linear Regression](https://github.com/mirsazzathossain/CSE421-Machine-Learning/blob/main/linear-regression.ipynb)\n",
        "2. [Naive Bayes Classifier](https://github.com/mirsazzathossain/CSE421-Machine-Learning/blob/main/naive-bayes.ipynb)\n",
        "3. [Video Recording](https://drive.google.com/file/d/1z1dD2z8MX1dgEZbQhZ6419BymmRVM87m/view?usp=sharing)\n",
        "\n",
        "### **Problem Statement**\n",
        "\n",
        "In this assignment, you will use the [Wine Dataset](https://github.com/mirsazzathossain/CSE421-Machine-Learning/blob/main/datasets/wine.csv) to classify the origin of wines based on their physicochemical properties. The dataset contains 178 instances with 13 numeric attributes. The attributes are:\n",
        "\n",
        "1. Alcohol: Alcohol content of the wine (in % vol)\n",
        "2. Malic acid: Malic acid content of the wine (in g/l)\n",
        "3. Ash: Ash content of the wine (in g/l)\n",
        "4. Alcalinity of ash: Alcalinity of ash of the wine (in mEq/l)\n",
        "5. Magnesium: Magnesium content in the wine (in mg/l)\n",
        "6. Total phenols: Total phenols content of the wine (in g/l)\n",
        "7. Flavanoids: Flavanoids content of the wine (in g/l)\n",
        "8. Nonflavanoid phenols: Nonflavanoids phenols content of the wine (in g/l)\n",
        "9. Proanthocyanins: Proanthocyanins content of the wine (in g/l)\n",
        "10. Color intensity: Color intensity of the wine (in OD absorbance units)\n",
        "11. Hue: Hue of the wine (in 1-10 scale)\n",
        "12. OD280/OD315 of diluted wines: OD280/OD315 of diluted wines (in OD absorbance units)\n",
        "13. Proline: Proline content in the wine (in mg/l)\n",
        "\n",
        "The dataset is divided into three classes, with 59, 71, and 48 instances each, corresponding to wines from three different origins: Barolo, Grignolino, and Barbera. 0 denotes Barolo, 1 denotes Grignolino, and 2 denotes Barbera.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDhxMN1tSMb_"
      },
      "source": [
        "### **Import necessary libraries**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l3QB9ljoSMb_"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CnRp8KySMcA"
      },
      "source": [
        "### **Load the dataset**\n",
        "\n",
        "Download the csv file form [here](https://github.com/mirsazzathossain/CSE421-Machine-Learning/blob/main/datasets/wine.csv), don't use dataset from any other source. Load the dataset using pandas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PlWROhU-SMcA"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q80GYyWSMcB"
      },
      "source": [
        "### **Data Preprocessing**\n",
        "\n",
        "#### **Check the information and statistical summary of the dataset**\n",
        "\n",
        "Check if there is any missing value in the dataset. You can use `df.info()` to get the information about the dataset and `df.describe()` to get the statistical summary of the dataset. Observe we didn't got 178 entries for all the columns, which means there are some missing values in the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XdjNR_1MSMcB"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBX8-eT0SMcB"
      },
      "source": [
        "#### **Handle the missing values**\n",
        "\n",
        "There are several ways to handle missing values as we discussed in the class. Those are:\n",
        "\n",
        "1. Delete the rows with missing values\n",
        "2. Fill the missing values with mean, median, mode\n",
        "3. Fill the missing values with a constant value (maximum or minimum)\n",
        "\n",
        "Another way to fill the missing values is to use machine learning algorithms. In this assignment, you have to fill the missing values with **linear regression**. You can follow the steps below to fill the missing values:\n",
        "\n",
        "1. Plot a pair plot of the dataset using `sns.pairplot()`. You can use `hue` parameter to differentiate between the classes. Observe the plot and find out the column where the missing values are present and the column which is most correlated to the column with missing values. To justify your answer, you can use `df.corr()` to find the correlation between the columns and `sns.heatmap()` to plot the correlation matrix.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fD_e_1nkSMcC"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB5jaK9nSMcC"
      },
      "source": [
        "2. Let's say column `A` has missing values and column `B` is most correlated to column `A`. Now, you have to find the linear regression line between column `A` and column `B`. To do that, you have to create a new dataframe with two columns, column `A` and column `B`. Then, you have to divide the dataframe into two parts, one without missing values (let's call it `df_train`) and another with missing values (let's call it `df_test`). Now, you have to find the linear regression line between column `A` and column `B` using `df_train` and then predict the missing values of column `A` using `df_test`. You can use `df_test = new_df[new_df['A'].isnull()]` to get the dataframe `df_test` and `df_train = new_df.dropna()` to get the dataframe `df_train`. Create `df_train` and `df_test` and print them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XMAdvBYTSMcC"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nbl9DlQFSMcC"
      },
      "source": [
        "3. Train a linear regression model using `df_train` and predict the missing values of column `A` using `df_test`. You can use `from sklearn.linear_model import LinearRegression` to import the linear regression model. Create the model, train the model, and predict the missing values using the model. Store the predicted values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Kpyj7Ow6SMcC"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaE9d-j2SMcC"
      },
      "source": [
        "4. Now, fill the missing values of column `A` with the predicted values in original dataframe `df` and check if there is any missing value left.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gjYfBwXKSMcD"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk8uwgg-SMcD"
      },
      "source": [
        "#### **Check for outliers**\n",
        "\n",
        "Now that you have handled the missing values, check for outliers in the dataset. You can use boxplot to check for outliers. If you find any outliers, remove them. If you wonder how to get the indices of the outliers, without plotting the boxplot, you can use the following code snippet:\n",
        "\n",
        "```python\n",
        "def get_outliers(df, col):\n",
        "    q1 = df[col].quantile(0.25)\n",
        "    q3 = df[col].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    return df[(df[col] < lower_bound) | (df[col] > upper_bound)].index\n",
        "```\n",
        "\n",
        "Call the function `get_outliers(df, col)` with the dataframe and the column name as parameters to get the indices of the outliers and then replace the outliers with the median of the column. You can use `df[col].median()` to get the median of the column. Loop through all the columns and replace the outliers with median value. To get the list of all the columns, you can use `df.columns`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tda42BXfSMcD"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLhvt6llSMcD"
      },
      "source": [
        "### **Model Building**\n",
        "\n",
        "After preprocessing the dataset, we will now build the model. In this assignment, you will use **Decision Tree** algorithm to classify the origin of the wines.\n",
        "\n",
        "A decision tree is a flowchart-like structure in which each internal node represents a feature (or attribute), each branch represents a decision rule, and each leaf node represents the outcome.\n",
        "\n",
        "The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner call recursive partitioning. This flowchart-like structure helps you in decision making. It's visualization like a flowchart diagram which easily mimics the human level thinking. That is why decision trees are easy to understand and interpret.\n",
        "\n",
        "![Decision Tree](https://images.datacamp.com/image/upload/v1677504957/decision_tree_for_heart_attack_prevention_2140bd762d.png)\n",
        "\n",
        "For a better understanding of decision tree, you can watch [this video](https://youtu.be/_L39rN6gz7Y?si=jj5_TyoloYdCVytJ) from StatQuest and/or read [this article](https://www.datacamp.com/tutorial/decision-tree-classification-python) from datacamp.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kgfIxG0SMcD"
      },
      "source": [
        "#### **Feature Selection**\n",
        "\n",
        "Before building the model, we have to select the features. You can use all the features or you can use some of the features. To select the features, you refer to the correlation matrix you plotted earlier. You can select the features which are most correlated to the target variable. You can also use `df.corr()['target']` to get the correlation of the features with the target variable and select the features which have correlation greater than 0.5 or 0.6. Selection of features is up to you. You can use all the features or you can use some of the features, **test the model with different features, and select the features which give the best result**. Then drop the other features from the dataset using `df.drop(['col1', 'col2', ...], axis=1)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "L7RqrkabSMcD"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjkWlCpXSMcD"
      },
      "source": [
        "#### **Split the dataset into features and target variable**\n",
        "\n",
        "Split the dataset into features and target variable. You can use `df.drop(['target'], axis=1)` to get the features and `df['target']` to get the target variable. Store the features in variable `X` and target variable in variable `y`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gdUXouCZSMcE"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyykpuWtSMcE"
      },
      "source": [
        "#### **Building Decision Tree Model and Evaluation**\n",
        "\n",
        "Now, you have to build the decision tree model. You can use `from sklearn.tree import DecisionTreeClassifier` to import the decision tree classifier. Create the model, train the model, and predict the target variable using the model. Initialize the classifier model. Use `gini` or `entropy` as the criterion and different values for `max_depth`.\n",
        "\n",
        "After building the model, you have to evaluate the model. You can use cross validation to evaluate the model. You can use `from sklearn.model_selection import cross_val_score` to import cross validation. Use `cross_val_score()` to evaluate the model. You can use `cv=5` to use 5-fold cross validation. Print the accuracy of the model for different values of `max_depth` and `criterion` (i.e. gini or entropy) also different sets of features. Select the best model based on the accuracy. To call `cross_val_score()`, you can use the following code snippet:\n",
        "\n",
        "```python\n",
        "scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
        "```\n",
        "\n",
        "The above code snippet will return an array of accuracy scores for 5-fold cross validation. You can print the accuracy scores using `print(scores)` and the mean accuracy score using `print(scores.mean())`. Select the best model based on the accuracy.\n",
        "\n",
        "For more information on cross validation, you can refer to [this video](https://youtu.be/fSytzGwwBVw?si=RIazXC9inPcQuGrf) from StatQuest.\n",
        "\n",
        "**Note:** Bonus marks may be awarded to the students with the best score in the class. 😉\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZpsLbUIWSMcE"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_smXAu3DSMcE"
      },
      "source": [
        "#### **Devide the dataset into train and test set**\n",
        "\n",
        "After selecting the best model, you have to divide the dataset into train and test set. You can use `from sklearn.model_selection import train_test_split` to import train test split. Use `train_test_split()` to divide the dataset into train and test set. You can use `test_size=0.2` to use 20% of the dataset as test set. Print the shape of train and test set to check if the dataset is divided correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OfcMpQu3SMcE"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRSbfe3DSMcE"
      },
      "source": [
        "#### **Train the model and make prediction**\n",
        "\n",
        "Now, you have to train the model using the train set. You can use `clf.fit(X_train, y_train)` to train the model. After training the model, you have to make prediction on the test set. You can use `clf.predict(X_test)` to make prediction on the test set. Store the predicted values in variable `y_pred`. Print the accuracy of the model using `accuracy_score(y_test, y_pred)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rWFax0XeSMcE"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0Q8dDcXSMcF"
      },
      "source": [
        "#### **Classification Report**\n",
        "\n",
        "You can use `from sklearn.metrics import classification_report` to import classification report. Use `classification_report(y_test, y_pred)` to print the classification report.\n",
        "\n",
        "Also, find out the confusion matrix using `from sklearn.metrics import confusion_matrix` and plot the confusion matrix using `sns.heatmap()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eY4QshBFSMcF"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO0VDoipSMcF"
      },
      "source": [
        "#### **Visualize the Decision Tree**\n",
        "\n",
        "Use the following code snippet to visualize the decision tree:\n",
        "\n",
        "```python\n",
        "from six import StringIO\n",
        "from IPython.display import Image\n",
        "from sklearn.tree import export_graphviz\n",
        "import pydotplus\n",
        "\n",
        "dot_data = StringIO()\n",
        "export_graphviz(clf, out_file=dot_data, filled=True, rounded=True, special_characters=True, feature_names = feature_cols, class_names=['0', '1', '2'])\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
        "graph.write_png('wine.png')\n",
        "Image(graph.create_png())\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tW6a5nQ7SMcF"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}